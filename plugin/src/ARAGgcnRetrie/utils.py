# --- START OF FILE utils.py ---

from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Any, Callable, Dict
import torch
import torch.nn.functional as F
import json
import ast

def normalize_item_data(item: dict) -> dict:
    item_id = str(item.get('item_id', item.get('sub_item_id', 'unknown_id')))
    name = item.get('title') or item.get('name') or item.get('business_name') or f"Item {item_id}"
    raw_desc = item.get('description') or item.get('text') or ""
    if isinstance(raw_desc, list):
        clean_desc = " ".join([str(x) for x in raw_desc])
    else:
        clean_desc = str(raw_desc)
    category = item.get('categories') or item.get('type') or "General"
    if isinstance(category, list):
        category = ", ".join(category)

    return {
        "item_id": item_id,
        "name": name,
        "description": clean_desc,
        "category": str(category),
        "original_data": item 
    }

def get_batch_embeddings(texts: List[str], embedding_function: Callable) -> torch.Tensor:
    vectors = embedding_function.embed_documents(texts)
    return torch.tensor(vectors)

def generate_graph_context_string(
    user_history_ids: List[str],
    gcn_embeddings: Dict[str, torch.Tensor],
    candidate_list: List[dict],
    top_k_neighbors: int = 5
) -> str:
    """
    HÃ m nÃ y Ä‘Ã³ng vai trÃ² 'GCN Context':
    NÃ³ nhÃ¬n vÃ o Ä‘á»“ thá»‹ (Embedding) Ä‘á»ƒ tÃ¬m cÃ¡c item cÃ³ cáº¥u trÃºc máº¡ng lÆ°á»›i gáº§n nháº¥t vá»›i user,
    sau Ä‘Ã³ tráº£ vá» má»™t chuá»—i vÄƒn báº£n mÃ´ táº£ cÃ¡c item nÃ y Ä‘á»ƒ lÃ m gá»£i Ã½ cho RAG.
    """
    if not gcn_embeddings or not user_history_ids:
        return ""

    # 1. Táº¡o User Embedding tá»« lá»‹ch sá»­ (Mean Pooling cÃ¡c node GCN)
    user_vecs = []
    for uid in user_history_ids:
        if uid in gcn_embeddings:
            user_vecs.append(gcn_embeddings[uid])
    
    if not user_vecs:
        return ""
    
    user_emb = torch.stack(user_vecs).mean(dim=0).unsqueeze(0) # (1, dim)

    # 2. TÃ¬m cÃ¡c Item trong Candidate List cÃ³ vector GCN gáº§n nháº¥t (Graph Similarity)
    # LÆ°u Ã½: ÄÃ¢y chá»‰ lÃ  tÃ¬m 'gá»£i Ã½' tá»« Ä‘á»“ thá»‹ Ä‘á»ƒ táº¡o context, chÆ°a pháº£i final result
    candidates_with_gcn = []
    for item in candidate_list:
        iid = str(item['item_id'])
        if iid in gcn_embeddings:
            candidates_with_gcn.append((item, gcn_embeddings[iid]))
    
    if not candidates_with_gcn:
        return ""

    cand_tensor = torch.stack([x[1] for x in candidates_with_gcn])
    
    # TÃ­nh cosine similarity trong khÃ´ng gian GCN
    scores = F.cosine_similarity(user_emb, cand_tensor)
    
    # Láº¥y Top K items theo máº¡ng lÆ°á»›i Ä‘á»“ thá»‹
    top_indices = torch.topk(scores, k=min(top_k_neighbors, len(candidates_with_gcn))).indices.tolist()
    
    suggested_items = [candidates_with_gcn[i][0] for i in top_indices]
    
    # 3. Táº¡o cÃ¢u vÄƒn Context (Prompt Expansion)
    item_names = [f"'{item['name'] or item['title']}' ({item['category']})" for item in suggested_items]
    
    context_str = (
        f"Graph Analysis: The user's interaction network strongly aligns with these items: {', '.join(item_names)}. "
        "Items sharing similar structural patterns in the graph should be prioritized."
    )
    
    return context_str

def perform_rag_retrieval(
    augmented_query: str,
    candidate_list: List[dict],
    embedding_function: Callable,
    top_k: int = 7
) -> List[dict]:
    """
    HÃ m RAG thuáº§n tÃºy:
    DÃ¹ng cÃ¢u query (Ä‘Ã£ Ä‘Æ°á»£c bÆ¡m GCN Context) Ä‘á»ƒ tÃ¬m kiáº¿m Semantic Similarity.
    """
    sims = []
    
    query_vec = embedding_function.embed_query(augmented_query)
    
    item_texts = [str(item) for item in candidate_list]
    item_vectors = embedding_function.embed_documents(item_texts) 
    
    for item, item_vec in zip(candidate_list, item_vectors):
        sim = cosine_similarity([item_vec], [query_vec])[0][0]
        sims.append((item, sim))
    
    sims.sort(key = lambda x: x[1], reverse = True)
    top_k_list = [item for item, sim in sims[:top_k]]
    
    return top_k_list


def print_agent_step(agent_name: str, message: str, data: Any = None):
    header = f"=== [AGENT: {agent_name.upper()}] ==="
    print(f"\n\033[94m{header}\033[0m") 
    print(f"ğŸ’¬ {message}")
    if data:
        if isinstance(data, list):
            print(f"ğŸ“Š Items count: {len(data)}")
            for i, item in enumerate(data[:3]):
                print(f"   - Item {i+1}: {item}")
            if len(data) > 3: print("   ...")
        else:
            print(f"ğŸ“ Data: {data}")
    print("\033[94m" + "="*len(header) + "\033[0m\n")

def get_gcn_latent_interests(user_history_ids, gcn_embeddings, candidate_list):
    """
    TÃ¬m ra 'Gu' cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn cÃ¡c item lÃ¢n cáº­n trong Ä‘á»“ thá»‹.
    """
    if not gcn_embeddings or not user_history_ids:
        return "No specific graph patterns detected."

    # 1. TÃ¬m User Embedding
    user_vecs = [gcn_embeddings[uid] for uid in user_history_ids if uid in gcn_embeddings]
    if not user_vecs: return "No graph data available."
    user_emb = torch.stack(user_vecs).mean(dim=0)

    # 2. TÃ¬m cÃ¡c Item 'hÃ ng xÃ³m' trong Ä‘á»“ thá»‹
    cand_ids = [str(item['item_id']) for item in candidate_list if str(item['item_id']) in gcn_embeddings]
    if not cand_ids: return "New user profile with limited graph connections."
    
    cand_tensor = torch.stack([gcn_embeddings[iid] for iid in cand_ids])
    sims = torch.nn.functional.cosine_similarity(user_emb.unsqueeze(0), cand_tensor)
    
    top_k = torch.topk(sims, k=min(5, len(cand_ids)))
    
    # 3. Láº¥y Ä‘áº·c Ä‘iá»ƒm chung cá»§a cÃ¡c item nÃ y (Category, Style)
    suggested_features = []
    for idx in top_k.indices:
        item_id = cand_ids[idx]
        item_data = next(i for i in candidate_list if str(i['item_id']) == item_id)
        suggested_features.append(f"{item_data.get('category')} ({item_data.get('name') or item_data.get('title')} )")

    return f"Structural community patterns suggest a preference for: {', '.join(suggested_features)}."